{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StockVision - Phase 2 Model Training\n",
    "\n",
    "Dieses Notebook trainiert **zwei Modelle** auf dem vorbereiteten AAPL-Feature-Datensatz:\n",
    "\n",
    "- **TensorFlow LSTM** fur Preis-Regression (`predicted_price`)\n",
    "- **XGBoost Classifier** fur Trend-Klassifikation (`SELL/HOLD/BUY`)\n",
    "\n",
    "Outputs:\n",
    "- `models/AAPL_price/`\n",
    "- `models/AAPL_trend/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Imports\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPU available:\", len(tf.config.list_physical_devices(\"GPU\")) > 0)\n",
    "\n",
    "# ==========================================\n",
    "# Paths and data loading\n",
    "# ==========================================\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    BASE_DIR = Path(\"..\").resolve()\n",
    "else:\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "DATA_PROCESSED = BASE_DIR / \"data\" / \"processed\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "PRICE_DIR = MODELS_DIR / \"AAPL_price\"\n",
    "TREND_DIR = MODELS_DIR / \"AAPL_trend\"\n",
    "PRICE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TREND_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TICKER = \"AAPL\"\n",
    "SEQUENCE_LENGTH = 90\n",
    "\n",
    "# Prefer direction dataset if present, fallback to features dataset\n",
    "candidate_paths = [\n",
    "    DATA_PROCESSED / f\"{TICKER}_features_direction.csv\",\n",
    "    DATA_PROCESSED / f\"{TICKER}_features.csv\",\n",
    "]\n",
    "for data_path in candidate_paths:\n",
    "    if data_path.exists():\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(f\"No processed feature file found in {DATA_PROCESSED}\")\n",
    "\n",
    "print(\"Loading:\", data_path)\n",
    "df = pd.read_csv(data_path, index_col=0, parse_dates=True).sort_index()\n",
    "print(\"Shape before cleaning:\", df.shape)\n",
    "\n",
    "# Ensure labels exist (in case notebook 01 wasn't rerun yet)\n",
    "if \"Next_Close\" not in df.columns:\n",
    "    df[\"Next_Close\"] = df[\"Close\"].shift(-1)\n",
    "if \"Direction_Binary\" not in df.columns:\n",
    "    df[\"Direction_Binary\"] = (df[\"Next_Close\"] > df[\"Close\"]).astype(float)\n",
    "if \"Return_Next\" not in df.columns:\n",
    "    df[\"Return_Next\"] = df[\"Close\"].pct_change().shift(-1)\n",
    "if \"Direction_3Class\" not in df.columns:\n",
    "    threshold = 0.005\n",
    "    df[\"Direction_3Class\"] = pd.cut(\n",
    "        df[\"Return_Next\"],\n",
    "        bins=[-np.inf, -threshold, threshold, np.inf],\n",
    "        labels=[0, 1, 2],\n",
    "    ).astype(float)\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_cols = {\"Direction_Binary\", \"Direction_3Class\", \"Next_Close\", \"Return_Next\"}\n",
    "feature_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "\n",
    "if \"Close\" not in feature_cols:\n",
    "    raise ValueError(\"Close must be present in feature columns for LSTM target regression.\")\n",
    "\n",
    "print(\"Feature count:\", len(feature_cols))\n",
    "print(\"Sample features:\", feature_cols[:10])\n",
    "print(\"trained_until:\", df.index.max().date())\n",
    "\n",
    "\n",
    "def temporal_split_indices(n, train_ratio=0.70, val_ratio=0.15):\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    return train_end, val_end\n",
    "\n",
    "\n",
    "def safe_mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MODEL 1: TensorFlow LSTM (Price Regression)\n",
    "# ==========================================\n",
    "\n",
    "lstm_df = df[feature_cols + [\"Close\"]].copy()\n",
    "# Remove duplicate Close column if it was included twice\n",
    "lstm_df = lstm_df.loc[:, ~lstm_df.columns.duplicated()].dropna().copy()\n",
    "\n",
    "n_total = len(lstm_df)\n",
    "train_end, val_end = temporal_split_indices(n_total)\n",
    "train_df = lstm_df.iloc[:train_end].copy()\n",
    "val_df = lstm_df.iloc[train_end:val_end].copy()\n",
    "test_df = lstm_df.iloc[val_end:].copy()\n",
    "\n",
    "print(\"LSTM splits:\", len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "# Fit scaler only on train features\n",
    "feature_scaler = MinMaxScaler()\n",
    "feature_scaler.fit(train_df[feature_cols])\n",
    "\n",
    "\n",
    "def make_sequences(split_df, feature_scaler, feature_cols, sequence_length):\n",
    "    X_scaled = feature_scaler.transform(split_df[feature_cols])\n",
    "    y = split_df[\"Close\"].to_numpy(dtype=np.float32)\n",
    "    dates = split_df.index.to_numpy()\n",
    "    X_seq, y_seq, d_seq = [], [], []\n",
    "    for i in range(sequence_length, len(split_df)):\n",
    "        X_seq.append(X_scaled[i-sequence_length:i])\n",
    "        y_seq.append(y[i])\n",
    "        d_seq.append(dates[i])\n",
    "    return np.asarray(X_seq, dtype=np.float32), np.asarray(y_seq, dtype=np.float32), np.asarray(d_seq)\n",
    "\n",
    "X_train, y_train, d_train = make_sequences(train_df, feature_scaler, feature_cols, SEQUENCE_LENGTH)\n",
    "X_val, y_val, d_val = make_sequences(val_df, feature_scaler, feature_cols, SEQUENCE_LENGTH)\n",
    "X_test, y_test, d_test = make_sequences(test_df, feature_scaler, feature_cols, SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"LSTM sequence shapes:\")\n",
    "print(\"  X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"  X_val:  \", X_val.shape, \"y_val:  \", y_val.shape)\n",
    "print(\"  X_test: \", X_test.shape, \"y_test: \", y_test.shape)\n",
    "\n",
    "if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:\n",
    "    raise ValueError(\n",
    "        \"Not enough rows after cleaning for 90-day sequences in all splits. \"\n",
    "        \"Reduce SEQUENCE_LENGTH or provide more data.\"\n",
    "    )\n",
    "\n",
    "lstm_model = keras.Sequential([\n",
    "    layers.Input(shape=(SEQUENCE_LENGTH, len(feature_cols))),\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"mse\",\n",
    "    metrics=[keras.metrics.RootMeanSquaredError(name=\"rmse\"), keras.metrics.MeanAbsolutePercentageError(name=\"mape\")]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-5),\n",
    "]\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=60,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "y_pred_test = lstm_model.predict(X_test, verbose=0).reshape(-1)\n",
    "rmse = float(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "mape = safe_mape(y_test, y_pred_test)\n",
    "\n",
    "print(f\"LSTM Test RMSE: {rmse:.4f}\")\n",
    "print(f\"LSTM Test MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Save artifacts\n",
    "lstm_model.save(PRICE_DIR / \"model.keras\")\n",
    "with open(PRICE_DIR / \"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"feature_scaler\": feature_scaler, \"features\": feature_cols}, f)\n",
    "\n",
    "price_metadata = {\n",
    "    \"type\": \"lstm_regression\",\n",
    "    \"ticker\": TICKER,\n",
    "    \"sequence_length\": SEQUENCE_LENGTH,\n",
    "    \"features\": feature_cols,\n",
    "    \"rmse\": rmse,\n",
    "    \"mape\": mape,\n",
    "    \"trained_until\": str(lstm_df.index.max().date()),\n",
    "}\n",
    "(PRICE_DIR / \"metadata.json\").write_text(json.dumps(price_metadata, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved LSTM artifacts to:\", PRICE_DIR)\n",
    "\n",
    "# Quick plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_test, label=\"Actual\", linewidth=1.5)\n",
    "plt.plot(y_pred_test, label=\"Predicted\", linewidth=1.5)\n",
    "plt.title(\"LSTM Price Regression (Test Split)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# MODEL 2: XGBoost Classifier (Trend)\n",
    "# ==========================================\n",
    "\n",
    "trend_df = df[feature_cols + [\"Direction_3Class\"]].copy().dropna().copy()\n",
    "trend_df[\"Direction_3Class\"] = trend_df[\"Direction_3Class\"].astype(int)\n",
    "\n",
    "n_total = len(trend_df)\n",
    "train_end, val_end = temporal_split_indices(n_total)\n",
    "train_cls = trend_df.iloc[:train_end].copy()\n",
    "val_cls = trend_df.iloc[train_end:val_end].copy()\n",
    "test_cls = trend_df.iloc[val_end:].copy()\n",
    "\n",
    "X_train_cls = train_cls[feature_cols]\n",
    "y_train_cls = train_cls[\"Direction_3Class\"]\n",
    "X_val_cls = val_cls[feature_cols]\n",
    "y_val_cls = val_cls[\"Direction_3Class\"]\n",
    "X_test_cls = test_cls[feature_cols]\n",
    "y_test_cls = test_cls[\"Direction_3Class\"]\n",
    "\n",
    "print(\"XGB splits:\", len(X_train_cls), len(X_val_cls), len(X_test_cls))\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective=\"multi:softprob\",\n",
    "    num_class=3,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train_cls,\n",
    "    y_train_cls,\n",
    "    eval_set=[(X_val_cls, y_val_cls)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "y_pred_cls = xgb_model.predict(X_test_cls)\n",
    "accuracy = float(accuracy_score(y_test_cls, y_pred_cls))\n",
    "cm = confusion_matrix(y_test_cls, y_pred_cls, labels=[0, 1, 2])\n",
    "\n",
    "print(f\"XGB Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion matrix [rows=true, cols=pred] for labels [0,1,2]:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"SELL\", \"HOLD\", \"BUY\"], yticklabels=[\"SELL\", \"HOLD\", \"BUY\"])\n",
    "plt.title(\"XGBoost Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save artifacts\n",
    "xgb_model.save_model(str(TREND_DIR / \"xgb_model.json\"))\n",
    "trend_metadata = {\n",
    "    \"type\": \"xgboost_classifier\",\n",
    "    \"ticker\": TICKER,\n",
    "    \"features\": feature_cols,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"label_mapping\": {\"0\": \"SELL\", \"1\": \"HOLD\", \"2\": \"BUY\"},\n",
    "    \"trained_until\": str(trend_df.index.max().date()),\n",
    "}\n",
    "(TREND_DIR / \"metadata.json\").write_text(json.dumps(trend_metadata, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved XGBoost artifacts to:\", TREND_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}